{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78eaff7e-dc99-443e-8e67-b1efad9c0dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "!pip install ultralytics\n",
    "!pip install shapely\n",
    "\n",
    "!pip uninstall opencv-python\n",
    "!pip install opencv-python==4.10.0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719061a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2 as cv\n",
    "from shapely.geometry import Point, Polygon\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387f6eb-640d-479d-ab90-e5232722287c",
   "metadata": {},
   "source": [
    "## Project utils elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58a7406-3ac9-496f-a5b0-0ca8492d250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_rgb(img_path):\n",
    "    img = cv.imread(img_path, cv.IMREAD_COLOR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58f2f59-fb35-44f3-b833-be21a5c99814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parking_spaces_polygons():\n",
    "    \"\"\"\n",
    "    This function obtains the delimitations of the first 10 parking spaces on the right-hand side\n",
    "    of the image, as polygons\n",
    "    \"\"\"\n",
    "    \n",
    "    parking_spaces_polygons = [\n",
    "        [[1587, 877], [1760, 982], [1763, 1050], [1558, 1036]],\n",
    "        [[1451, 792], [1585, 879], [1556, 1038], [1400, 918]],\n",
    "        [[1335, 714], [1450, 790], [1398, 916],  [1269, 823]],\n",
    "        [[1238, 657], [1335, 716], [1269, 824],  [1164, 745]],\n",
    "        [[1158, 602], [1240, 654], [1164, 746],  [1077, 679]],\n",
    "        [[1091, 556], [1158, 602], [1077, 679],  [1002, 626]],\n",
    "        [[1026, 520], [1086, 557], [999, 625],   [938, 576]],\n",
    "        [[968, 485],  [1026, 518], [940, 576],   [884, 534]],\n",
    "        [[919, 454],  [972, 485],  [886, 534],   [835, 499]],\n",
    "        [[875, 429],  [918, 456],  [834, 499],   [794, 466]],\n",
    "    ]\n",
    "\n",
    "    image_height = 1050\n",
    "    for i in range(len(parking_spaces_polygons)):\n",
    "        for j in range(len(parking_spaces_polygons[i])):\n",
    "            parking_spaces_polygons[i][j][1] = image_height - parking_spaces_polygons[i][j][1] # We \"invert\" the y axis in order to work with the \"shapely\" library\n",
    "    \n",
    "    parking_spaces_polygons = [Polygon(poly_coords) for poly_coords in parking_spaces_polygons]\n",
    "\n",
    "    return parking_spaces_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beab42ff-16e3-4279-816a-c248360fcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_spaces_polygons = get_parking_spaces_polygons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52cfcd46-3d72-4c4f-88d2-a4e7399cc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detections_bounding_boxes(img, model):\n",
    "    \"\"\"This function is only for tasks 1 and 2\"\"\"\n",
    "    \n",
    "    bounding_boxes = []\n",
    "    \n",
    "    detections_whole_image = yolov9_predict(model, img)\n",
    "\n",
    "    # We crop the last 5 parking spaces from the initial image so that the model\n",
    "    # will be able to more easily detect farther away cars\n",
    "    h_top, h_bottom, w_left, w_right = 329, 679, 710, 1312\n",
    "    last_5_parking_spaces = img[h_top:h_bottom, w_left:w_right]\n",
    "    detections_last_5_parking_spaces = yolov9_predict(model, last_5_parking_spaces)\n",
    "    \n",
    "    for d in detections_whole_image:\n",
    "        bounding_box_coords = [int(coord) for coord in d.boxes.xyxy.tolist()[0]]\n",
    "        bounding_boxes.append(bounding_box_coords)\n",
    "    for d in detections_last_5_parking_spaces:\n",
    "        bounding_box_coords = [int(coord) for coord in d.boxes.xyxy.tolist()[0]]\n",
    "\n",
    "        # We want to translate the coordinates of the bounding boxes from the cropped image to coordinates in the initial image \n",
    "        bounding_box_coords[0] = bounding_box_coords[0] + w_left\n",
    "        bounding_box_coords[1] = bounding_box_coords[1] + h_top\n",
    "        bounding_box_coords[2] = bounding_box_coords[2] + w_left\n",
    "        bounding_box_coords[3] = bounding_box_coords[3] + h_top\n",
    "        \n",
    "        bounding_boxes.append(bounding_box_coords)\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e3e659-bfa8-419c-8c6f-a0ec8238a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parking_spaces_statuses(bounding_boxes, parking_spaces_polygons):\n",
    "    no_parking_spaces = 10\n",
    "    statuses = [0 for i in range(no_parking_spaces)]\n",
    "\n",
    "    image_height = 1050\n",
    "    pixels_to_verify = []\n",
    "    for box_coords in bounding_boxes:\n",
    "        box_coords[1], box_coords[3] = image_height - box_coords[1], image_height - box_coords[3] # We \"invert\" the y axis in order to work with the \"shapely\" library\n",
    "        box_center = (int((box_coords[0] + box_coords[2]) / 2), int((box_coords[1] + box_coords[3]) / 2))\n",
    "        \n",
    "        pixel = [box_center[0], int((box_center[1] + box_coords[3]) / 2)]\n",
    "        pixel = Point(pixel)\n",
    "        pixels_to_verify.append(pixel)\n",
    "\n",
    "    # We check if a parking space is occupied by verifying if a certain pixel from every detected object's bounding box\n",
    "    # falls within the parking space's delimitation\n",
    "    for pixel in pixels_to_verify:\n",
    "        for i in range(no_parking_spaces):\n",
    "            if parking_spaces_polygons[i].contains(pixel) or parking_spaces_polygons[i].touches(pixel):\n",
    "                statuses[i] = 1\n",
    "    \n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab007ec9-12ec-44d7-840f-f0e09b940225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_last_frame(video_path):\n",
    "    vc = cv.VideoCapture(video_path)\n",
    "    \n",
    "    last_frame_num = vc.get(cv.CAP_PROP_FRAME_COUNT) - 1\n",
    "    vc.set(cv.CAP_PROP_POS_FRAMES, last_frame_num)\n",
    "    \n",
    "    _, last_frame = vc.read()\n",
    "    \n",
    "    return last_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbd77e-376d-4d17-bacf-aad3487891be",
   "metadata": {},
   "source": [
    "## YOLOv9 object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba513592-f07f-4b34-b7bc-1439c3b9b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolov9_predict(model, img, conf=0.3, verbose=False):\n",
    "    detections = model.predict(img, classes=[2, 7], conf=conf, device='cuda:0', verbose=verbose)\n",
    "    detections = detections[0]\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bbaa1ea-b4a2-49c4-bd7c-6314378a4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov9e.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc6af6-7a5c-4cfb-bba3-aa4755e49c05",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9bad5cd-e8d4-4651-aacf-9dcc51aafb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1(path_to_task, path_to_task_res, model, parking_spaces_polygons):\n",
    "    no_contexts, no_scenarios_1, no_scenarios_2 = 15, 3, 4\n",
    "    \n",
    "    for i in range(1, no_contexts + 1):\n",
    "        scenario_zero = 0 if i <= 9 else ''\n",
    "        if i <= 10:\n",
    "            no_scenarios = no_scenarios_1\n",
    "        else:\n",
    "            no_scenarios = no_scenarios_2\n",
    "        \n",
    "        for j in range(1, no_scenarios + 1):\n",
    "            img_name = f'{scenario_zero}{i}_{j}.jpg'\n",
    "            img_path = f'{path_to_task}/{img_name}'\n",
    "            img = load_image_rgb(img_path)\n",
    "            \n",
    "            query_file_name = f'{scenario_zero}{i}_{j}_query.txt'\n",
    "            query_file_path = f'{path_to_task}/{query_file_name}'\n",
    "            with open(query_file_path) as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "                no_parking_spaces, parking_spaces = int(lines[0].rstrip()), []\n",
    "                for parking_space in lines[1:]:\n",
    "                    parking_space = int(parking_space.rstrip())\n",
    "                    parking_spaces.append(parking_space)\n",
    "            \n",
    "            detections_bounding_boxes = get_detections_bounding_boxes(img, model)\n",
    "            all_parking_spaces_statuses = get_parking_spaces_statuses(detections_bounding_boxes, parking_spaces_polygons)\n",
    "            \n",
    "            res_txt_name = f'{scenario_zero}{i}_{j}_predicted.txt'\n",
    "            res_txt_path = f'{path_to_task_res}/{res_txt_name}'\n",
    "            with open(res_txt_path, 'w') as file:\n",
    "                file.write(str(no_parking_spaces) + '\\n')\n",
    "                for space in parking_spaces[:-1]:\n",
    "                    file.write(str(space) + ' ' + str(all_parking_spaces_statuses[space - 1]) + '\\n')\n",
    "                file.write(str(parking_spaces[-1]) + ' ' + str(all_parking_spaces_statuses[parking_spaces[-1] - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70a34ba-4025-4a6e-8c8f-f1b7f3eac7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave the paths as they are or modify them accordingly\n",
    "path_to_task_1 = '../../../train/Task1/'\n",
    "path_to_task_1_res = 'Task1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9837f7-280c-44a1-93f2-e067c0629150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task1(path_to_task_1, path_to_task_1_res, model, parking_spaces_polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2a105-5f09-4d56-b87c-4c4b444dd98a",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5decfb24-31cc-4388-9d31-a441366139d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2(path_to_task, path_to_task_res, model, parking_spaces_polygons):\n",
    "    no_contexts = 15\n",
    "    \n",
    "    for i in range(1, no_contexts + 1):\n",
    "        context_zero = 0 if i <= 9 else ''\n",
    "        \n",
    "        video_name = f'{context_zero}{i}.mp4'\n",
    "        video_path = f'{path_to_task}/{video_name}'\n",
    "        video_last_frame = get_video_last_frame(video_path)\n",
    "        \n",
    "        detections_bounding_boxes = get_detections_bounding_boxes(video_last_frame, model)\n",
    "        all_parking_spaces_statuses = get_parking_spaces_statuses(detections_bounding_boxes, parking_spaces_polygons)\n",
    "        \n",
    "        res_txt_name = f'{context_zero}{i}_predicted.txt'\n",
    "        res_txt_path = f'{path_to_task_res}/{res_txt_name}'\n",
    "        with open(res_txt_path, 'w') as file:\n",
    "            for i in range(len(all_parking_spaces_statuses) - 1):\n",
    "                file.write(str(all_parking_spaces_statuses[i]) + '\\n')\n",
    "            file.write(str(all_parking_spaces_statuses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ed1980-8e4d-4be6-9a16-00ce9a0fb191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave the paths as they are or modify them accordingly\n",
    "path_to_task_2 = '../../../train/Task2/'\n",
    "path_to_task_2_res = 'Task2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3577f3b-c007-4ed1-a5c5-e437fef64840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task2(path_to_task_2, path_to_task_2_res, model, parking_spaces_polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d4e30-af6a-4dbe-9932-9694faa6fbc4",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edc87f49-1d75-4f0d-b3c8-6bc19dc4b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_score(bbox1, bbox2):\n",
    "    # Intersection area\n",
    "    intersection_width = min(bbox1[2], bbox2[2]) - max(bbox1[0], bbox2[0]) + 1\n",
    "    intersection_width = max(intersection_width, 0)\n",
    "    intersection_height = min(bbox1[3], bbox2[3]) - max(bbox1[1], bbox2[1]) + 1\n",
    "    intersection_height = max(intersection_height, 0)\n",
    "    intersection_area = intersection_width * intersection_height\n",
    "\n",
    "    # Union area\n",
    "    bbox1_area = (bbox1[2] - bbox1[0] + 1) * (bbox1[3] - bbox1[1] + 1)\n",
    "    bbox2_area = (bbox1[2] - bbox1[0] + 1) * (bbox1[3] - bbox1[1] + 1)\n",
    "    union_area = bbox1_area + bbox2_area - intersection_area\n",
    "\n",
    "    # Computing the IoU\n",
    "    iou = intersection_area / union_area\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db8789d6-b51b-4b58-b113-21eb29cbd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, input_txt_path, res_txt_path):\n",
    "    vc = cv.VideoCapture(video_path)\n",
    "    \n",
    "    res_file = open(res_txt_path, 'w')\n",
    "\n",
    "    # Processing the input file\n",
    "    input_file = open(input_txt_path)\n",
    "    lines = input_file.readlines()\n",
    "    initial_frame_info = lines[1].rstrip().split()\n",
    "    prev_bbox = [int(initial_frame_info[1]), int(initial_frame_info[2]), int(initial_frame_info[3]), int(initial_frame_info[4])]\n",
    "    prev_bbox_center = (int((prev_bbox[0] + prev_bbox[2]) / 2), int((prev_bbox[1] + prev_bbox[3]) / 2))\n",
    "    res_file.write(lines[0])\n",
    "    res_file.write(lines[1].rstrip() + '\\n')\n",
    "    input_file.close()\n",
    "\n",
    "    _, _ = vc.read() # We \"throw away\" the first frame, as we already have the info about it that we need, in the input file\n",
    "    current_frame = 0\n",
    "    \n",
    "    while vc.isOpened():\n",
    "        ret, frame = vc.read()\n",
    "        \n",
    "        if ret:\n",
    "            current_frame += 1\n",
    "    \n",
    "            detections = yolov9_predict(model, frame, conf=0.1)\n",
    "            bboxes = [[int(coord) for coord in bbox] for bbox in detections.boxes.xyxy.tolist()]\n",
    "\n",
    "            # Our sought bounding box and the previous bounding box should have the biggest IoU score out of all of the detected bounding boxes\n",
    "            max_iou_score, selected_bbox, bbox_updated = 0, None, False\n",
    "            for bbox in bboxes:\n",
    "                iou_score = compute_iou_score(prev_bbox, bbox)\n",
    "                if iou_score > max_iou_score:\n",
    "                    max_iou_score = iou_score\n",
    "                    selected_bbox = bbox.copy()\n",
    "            if max_iou_score > 0.45: # For when the algorithm detects the real bounding box. Otherwise we consider that it selected the bounding box of a different object\n",
    "                current_bbox = selected_bbox.copy()\n",
    "                bbox_updated = True\n",
    "            \n",
    "            if bbox_updated:\n",
    "                res_file.write(f'{current_frame} {current_bbox[0]} {current_bbox[1]} {current_bbox[2]} {current_bbox[3]}\\n')\n",
    "                prev_bbox = current_bbox.copy()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    res_file.close()\n",
    "    \n",
    "    vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe5d6216-e2bd-419a-8101-7621ae5741a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3(path_to_task, path_to_task_res, model):\n",
    "    no_contexts = 15\n",
    "\n",
    "    for i in range(1, no_contexts + 1):\n",
    "        context_zero = 0 if i <= 9 else ''\n",
    "        \n",
    "        video_name = f'{context_zero}{i}.mp4'\n",
    "        video_path = f'{path_to_task}/{video_name}'\n",
    "        input_txt_name = f'{context_zero}{i}.txt'\n",
    "        input_txt_path = f'{path_to_task}/{input_txt_name}'\n",
    "        \n",
    "        res_txt_name = f'{context_zero}{i}_predicted.txt'\n",
    "        res_txt_path = f'{path_to_task_res}/{res_txt_name}'\n",
    "\n",
    "        process_video(video_path, input_txt_path, res_txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eedf6071-701b-41ce-9c21-5a7d8b3b084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave the paths as they are or modify them accordingly\n",
    "path_to_task_3 = '../../../train/Task3/'\n",
    "path_to_task_3_res = 'Task3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "079bb6e6-6eeb-48f9-a870-508c144bed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3(path_to_task_3, path_to_task_3_res, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0684454-de26-402e-9c81-b53b9bbb2517",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e03b23a-2a3a-4814-819a-d59ef2ad0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_right_lane_polygons():\n",
    "    \"\"\"This function obtains the entire right lane and the beginning of the queuing area as polygons\"\"\"\n",
    "    \n",
    "    entire_lane_polygon = [[405, 218], [481, 221], [1548, 1049], [1293, 1049]]\n",
    "    starting_queue_area_polygon = [[405, 218], [481, 221], [559, 284], [459, 286]]\n",
    "\n",
    "    image_height = 1050\n",
    "    for i in range(len(entire_lane_polygon)):\n",
    "        # We \"invert\" the y axis in order to work with the \"shapely\" library\n",
    "        entire_lane_polygon[i][1] = image_height - entire_lane_polygon[i][1]\n",
    "        starting_queue_area_polygon[i][1] = image_height - starting_queue_area_polygon[i][1]\n",
    "    \n",
    "    entire_lane_polygon = Polygon(entire_lane_polygon)\n",
    "    starting_queue_area_polygon = Polygon(starting_queue_area_polygon)\n",
    "\n",
    "    return (entire_lane_polygon, starting_queue_area_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37ec98a5-a3d5-4e9c-b0db-86f16214d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_lane_polygons = get_right_lane_polygons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "694878ec-ec1d-49b8-8b96-4397a2d34cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_detections(model, frame):\n",
    "    # We will crop our image in order for the YOLO model to have a better chance at detecting farther away cars\n",
    "    h_top_1, h_bottom_1, w_left_1, w_right_1 = 191, 320, 390, 587\n",
    "    lane_cropping_1 = frame[h_top_1:h_bottom_1, w_left_1:w_right_1]\n",
    "    detections_lane_cropping_1 = yolov9_predict(model, lane_cropping_1, conf=0.15)\n",
    "    h_top_2, h_bottom_2, w_left_2, w_right_2 = 270, 450, 478, 753\n",
    "    lane_cropping_2 = frame[h_top_2:h_bottom_2, w_left_2:w_right_2]\n",
    "    detections_lane_cropping_2 = yolov9_predict(model, lane_cropping_2, conf=0.15)\n",
    "    h_top_3, h_bottom_3, w_left_3, w_right_3 = 360, 640, 600, 1045\n",
    "    lane_cropping_3 = frame[h_top_3:h_bottom_3, w_left_3:w_right_3]\n",
    "    detections_lane_cropping_3 = yolov9_predict(model, lane_cropping_3, conf=0.15)\n",
    "    \n",
    "    detections_whole_image = yolov9_predict(model, frame)\n",
    "\n",
    "    bounding_boxes = []\n",
    "\n",
    "    for d in detections_lane_cropping_1:\n",
    "        bounding_box_coords = [int(coord) for coord in d.boxes.xyxy.tolist()[0]]\n",
    "\n",
    "        # We want to translate the coordinates of the bounding boxes from the cropped image to coordinates in the initial image \n",
    "        bounding_box_coords[0] = bounding_box_coords[0] + w_left_1\n",
    "        bounding_box_coords[1] = bounding_box_coords[1] + h_top_1\n",
    "        bounding_box_coords[2] = bounding_box_coords[2] + w_left_1\n",
    "        bounding_box_coords[3] = bounding_box_coords[3] + h_top_1\n",
    "        \n",
    "        bounding_boxes.append(bounding_box_coords)\n",
    "    for d in detections_lane_cropping_2:\n",
    "        bounding_box_coords = [int(coord) for coord in d.boxes.xyxy.tolist()[0]]\n",
    "\n",
    "        # We want to translate the coordinates of the bounding boxes from the cropped image to coordinates in the initial image \n",
    "        bounding_box_coords[0] = bounding_box_coords[0] + w_left_2\n",
    "        bounding_box_coords[1] = bounding_box_coords[1] + h_top_2\n",
    "        bounding_box_coords[2] = bounding_box_coords[2] + w_left_2\n",
    "        bounding_box_coords[3] = bounding_box_coords[3] + h_top_2\n",
    "        \n",
    "        bounding_boxes.append(bounding_box_coords)\n",
    "    for d in detections_lane_cropping_3:\n",
    "        bounding_box_coords = [int(coord) for coord in d.boxes.xyxy.tolist()[0]]\n",
    "\n",
    "        # We want to translate the coordinates of the bounding boxes from the cropped image to coordinates in the initial image \n",
    "        bounding_box_coords[0] = bounding_box_coords[0] + w_left_3\n",
    "        bounding_box_coords[1] = bounding_box_coords[1] + h_top_3\n",
    "        bounding_box_coords[2] = bounding_box_coords[2] + w_left_3\n",
    "        bounding_box_coords[3] = bounding_box_coords[3] + h_top_3\n",
    "        \n",
    "        bounding_boxes.append(bounding_box_coords)\n",
    "    for d in detections_whole_image:\n",
    "        bounding_box_coords = [int(coord) for coord in d.boxes.xyxy.tolist()[0]]\n",
    "        bounding_boxes.append(bounding_box_coords)\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d80ffd4-a9eb-4edf-8823-48efeffbac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection_area(bbox1, bbox2):\n",
    "    intersection_width = min(bbox1[2], bbox2[2]) - max(bbox1[0], bbox2[0])\n",
    "    intersection_width = max(intersection_width, 0)\n",
    "    intersection_height = min(bbox1[3], bbox2[3]) - max(bbox1[1], bbox2[1])\n",
    "    intersection_height = max(intersection_height, 0)\n",
    "    \n",
    "    intersection_area = intersection_width * intersection_height\n",
    "    return intersection_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a84a159-4d74-4be5-8b1a-8dbf3c776f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_detections(lane_detections):\n",
    "    \"\"\"\n",
    "    Due to the function get_all_detections (more specifically, due to the cropping done by it), the YOLO model might\n",
    "    have detected certain cars more than one time. As such, we seek to remove duplicate detections\n",
    "    \"\"\"\n",
    "    \n",
    "    removed_detection = True\n",
    "    while removed_detection:\n",
    "        removed_detection = False\n",
    "        for i in range(len(lane_detections)):\n",
    "            width, height = lane_detections[i][2] - lane_detections[i][0], lane_detections[i][3] - lane_detections[i][1]\n",
    "            detection_area = width * height\n",
    "\n",
    "            # We check how much of the surface from bounding box i is overlapping with the surface \n",
    "            # of any other bounding box j and retain the maximum percentage of this value with report\n",
    "            # to the surface of bounding box i, after which if the percentage is greater than 0.75 (we\n",
    "            # truly have a duplicate bounding box, not the detections of two different cars intersecting),\n",
    "            # we treat bounding box i as a duplicate and remove it. If it is not greater than 0.75, we\n",
    "            # keep the bounding box i\n",
    "            max_overlapping_percentage = 0\n",
    "            for j in range(len(lane_detections)):\n",
    "                if j != i:\n",
    "                    intersection_area = get_intersection_area(lane_detections[i], lane_detections[j])\n",
    "                    overlapping_percentage = intersection_area / detection_area\n",
    "                    \n",
    "                    if overlapping_percentage > max_overlapping_percentage:\n",
    "                        max_overlapping_percentage = overlapping_percentage\n",
    "\n",
    "            if max_overlapping_percentage > 0.75:\n",
    "                lane_detections.pop(i)\n",
    "                removed_detection = True\n",
    "                break\n",
    "\n",
    "    return lane_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d3636af-7d55-4ff3-ae75-7e2273d5e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lane_detections(model, frame, entire_lane_polygon):\n",
    "    \"\"\"This function obtains the detections of all cars situated in the right lane of the image\"\"\"\n",
    "    \n",
    "    detections = get_all_detections(model, frame)\n",
    "    \n",
    "    image_height = 1050\n",
    "    # We \"invert\" the y axis of the detections in order to work with the \"shapely\" library\n",
    "    inverted_detections = [[d[0], image_height - d[1], d[2], image_height - d[3]] for d in detections]\n",
    "    \n",
    "    lane_detections = []\n",
    "    for i in range(len(inverted_detections)):\n",
    "        detection_center = [int((inverted_detections[i][0] + inverted_detections[i][2]) / 2), int((inverted_detections[i][1] + inverted_detections[i][3]) / 2)]\n",
    "        detection_center = Point(detection_center)\n",
    "        if entire_lane_polygon.contains(detection_center) or entire_lane_polygon.touches(detection_center):\n",
    "            lane_detections.append(detections[i])\n",
    "\n",
    "    lane_detections = remove_duplicate_detections(lane_detections)\n",
    "    # We want to obtain the detected cars from top to bottom order in their lane\n",
    "    lane_detections.sort(key=lambda index : index[1])\n",
    "\n",
    "    return lane_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "122e0311-b791-494f-9c93-45d83b6514d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_car_count_queuing(video_path, model, right_lane_polygons):\n",
    "    entire_lane_polygon, starting_queue_area_polygon = right_lane_polygons[0], right_lane_polygons[1]\n",
    "    \n",
    "    frame = get_video_last_frame(video_path)\n",
    "    lane_detections = get_lane_detections(model, frame, entire_lane_polygon)\n",
    "\n",
    "    cars_queuing = 0\n",
    "\n",
    "    if len(lane_detections) > 0:\n",
    "        image_height = 1050\n",
    "        center_first_detection = [int((lane_detections[0][0] + lane_detections[0][2]) / 2), int((lane_detections[0][1] + lane_detections[0][3]) / 2)]\n",
    "        center_first_detection[1] = image_height - center_first_detection[1] # We \"invert\" the y axis in order to work with the \"shapely\" library\n",
    "        center_first_detection = Point(center_first_detection)\n",
    "        \n",
    "        # If there is no car at the beginning area of the queue, then we treat as there being no cars queuing\n",
    "        if starting_queue_area_polygon.contains(center_first_detection) or starting_queue_area_polygon.touches(center_first_detection):\n",
    "            cars_queuing += 1\n",
    "\n",
    "            # We check if cars are queuing by comparing their coordinates\n",
    "            for i in range(1, len(lane_detections)):\n",
    "                if lane_detections[i][1] <= lane_detections[i - 1][3]:\n",
    "                    cars_queuing += 1\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    return cars_queuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8729778e-7d35-4724-b8ea-0b5e54897339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task4(path_to_task, path_to_task_res, model, right_lane_polygons):\n",
    "    no_contexts = 15\n",
    "\n",
    "    for i in range(1, no_contexts + 1):\n",
    "        context_zero = 0 if i <= 9 else ''\n",
    "        \n",
    "        video_name = f'{context_zero}{i}.mp4'\n",
    "        video_path = f'{path_to_task}/{video_name}'\n",
    "\n",
    "        car_count_queuing = get_car_count_queuing(video_path, model, right_lane_polygons)\n",
    "        \n",
    "        res_txt_name = f'{context_zero}{i}_predicted.txt'\n",
    "        res_txt_path = f'{path_to_task_res}/{res_txt_name}'\n",
    "\n",
    "        with open(res_txt_path, 'w') as file:\n",
    "            file.write(str(car_count_queuing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5874bc79-d1eb-423e-bf77-86d0561815a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave the paths as they are or modify them accordingly\n",
    "path_to_task_4 = '../../../train/Task4/'\n",
    "path_to_task_4_res = 'Task4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43b6a402-48e8-4d60-8766-33b9e3f0d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task4(path_to_task_4, path_to_task_4_res, model, right_lane_polygons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
